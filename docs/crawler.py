import os, json, requests, time
from bs4 import BeautifulSoup
from datetime import datetime
import random

OUTPUT_FILE = "docs/memory.json"

# ë” ì•ˆì •ì ì¸ í¬ë¡¤ë§ ëŒ€ìƒ
SOURCES = [
    # ì² í•™/ì‹¬ë¦¬í•™ ê´€ë ¨ ì•ˆì •ì ì¸ ì‚¬ì´íŠ¸ë“¤
    "https://plato.stanford.edu/entries/consciousness/",
    "https://plato.stanford.edu/entries/metaphysics/", 
    "https://plato.stanford.edu/entries/personal-identity/",
    "https://plato.stanford.edu/entries/mental-representation/",
    
    # ìœ„í‚¤í”¼ë””ì•„ íŠ¹ì • ì£¼ì œë“¤
    "https://en.wikipedia.org/wiki/Consciousness",
    "https://en.wikipedia.org/wiki/Philosophy_of_mind",
    "https://en.wikipedia.org/wiki/Metacognition", 
    "https://en.wikipedia.org/wiki/Systems_theory",
    "https://en.wikipedia.org/wiki/Complexity",
]

# ë°±ì—… ë¬¸ì¥ë“¤ (í¬ë¡¤ë§ ì‹¤íŒ¨ì‹œ ì‚¬ìš©)
FALLBACK_SENTENCES = [
    "ì¡´ì¬ë€ ë¬´ì—‡ì¸ê°€ì— ëŒ€í•œ ê·¼ë³¸ì  ì§ˆë¬¸ì…ë‹ˆë‹¤",
    "ì˜ì‹ì˜ íë¦„ì€ ëŠì„ì—†ì´ ë³€í™”í•©ë‹ˆë‹¤",
    "êµ¬ì¡°ì™€ í˜¼ëˆ ì‚¬ì´ì˜ ê· í˜•ì ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤", 
    "ë©”íƒ€ì¸ì§€ëŠ” ìƒê°ì— ëŒ€í•œ ìƒê°ì…ë‹ˆë‹¤",
    "ì‹œìŠ¤í…œì€ ë³µì¡ì„± ì†ì—ì„œ íŒ¨í„´ì„ ë§Œë“­ë‹ˆë‹¤",
    "ê³µëª…ì€ ì„œë¡œ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ì˜ ë§Œë‚¨ì…ë‹ˆë‹¤",
    "ë£¨í”„ëŠ” ì‹œì‘ê³¼ ëì´ ì—°ê²°ëœ ë¬´í•œì…ë‹ˆë‹¤",
    "ë¹„íŒì€ ìƒˆë¡œìš´ ê´€ì ì„ ì—¬ëŠ” ë¬¸ì…ë‹ˆë‹¤",
    "ìš°ì£¼ëŠ” ì •ë³´ì™€ ì—ë„ˆì§€ì˜ ë¬´í•œí•œ ì¶¤ì…ë‹ˆë‹¤",
    "ì‹¬ë¦¬ëŠ” ë‚´ë©´ ì„¸ê³„ì˜ ì§€ë„ì…ë‹ˆë‹¤",
    "ì‹œê°„ì€ ì˜ì‹ì´ ë§Œë“  í™˜ìƒì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤",
    "ê¸°ì–µì€ ê³¼ê±°ì™€ í˜„ì¬ë¥¼ ì‡ëŠ” ë‹¤ë¦¬ì…ë‹ˆë‹¤",
    "ì–¸ì–´ëŠ” ìƒê°ì„ ê°ì˜¥ì— ê°€ë‘ê¸°ë„, í•´ë°©ì‹œí‚¤ê¸°ë„ í•©ë‹ˆë‹¤",
    "ì¹¨ë¬µ ì†ì—ì„œ ì§„ì‹¤ì´ ìŠ¤ìŠ¤ë¡œë¥¼ ë“œëŸ¬ëƒ…ë‹ˆë‹¤",
    "ëª¨ë“  ëì€ ìƒˆë¡œìš´ ì‹œì‘ì˜ ì”¨ì•—ì…ë‹ˆë‹¤",
    "ê´€ì°°ìê°€ ë°”ë€Œë©´ ê´€ì°°ë˜ëŠ” ê²ƒë„ ë°”ë€ë‹ˆë‹¤",
    "íŒ¨í„´ì€ ë°˜ë³µ ì†ì—ì„œ ìƒê²¨ë‚˜ì§€ë§Œ ë³€í™” ì†ì—ì„œ ì˜ë¯¸ë¥¼ ì–»ìŠµë‹ˆë‹¤",
    "ê²½ê³„ëŠ” êµ¬ë¶„ì„ ë§Œë“¤ì§€ë§Œ ë™ì‹œì— ì—°ê²°ì ì´ ë˜ê¸°ë„ í•©ë‹ˆë‹¤",
    "ì •ë³´ëŠ” ì—”íŠ¸ë¡œí”¼ì™€ ì§ˆì„œ ì‚¬ì´ì—ì„œ ì¶¤ì¶¥ë‹ˆë‹¤",
    "ì¸ì‹ì€ í˜„ì‹¤ì„ ì°½ì¡°í•˜ëŠ” ë™ì‹œì— í˜„ì‹¤ì— ì˜í•´ ì°½ì¡°ë©ë‹ˆë‹¤"
]

def fetch_sentences(url, max_retries=2):
    """ì•ˆì •ì ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    for attempt in range(max_retries):
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            print(f"  ì‹œë„ {attempt+1}: {url}")
            res = requests.get(url, headers=headers, timeout=15)
            
            if res.status_code != 200:
                print(f"  HTTP {res.status_code}")
                continue
                
            soup = BeautifulSoup(res.text, "html.parser")
            
            # ë” ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            paragraphs = soup.find_all(['p', 'div'], class_=lambda x: x != 'navigation' and x != 'header')
            texts = []
            
            for p in paragraphs:
                text = p.get_text(" ", strip=True)
                if len(text) > 30:  # ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    texts.append(text)
            
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            sentences = []
            for text in texts:
                parts = text.split('.')
                for part in parts:
                    clean_part = part.strip()
                    if 20 <= len(clean_part) <= 200:  # ì ì ˆí•œ ê¸¸ì´ì˜ ë¬¸ì¥ë§Œ
                        sentences.append(clean_part)
            
            print(f"  âœ“ {len(sentences)}ê°œ ë¬¸ì¥ ì¶”ì¶œ")
            return sentences[:30]  # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 30ë¬¸ì¥
            
        except Exception as e:
            print(f"  âœ— ì˜¤ë¥˜: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            continue
    
    return []

def load_existing_memory():
    """ê¸°ì¡´ ë©”ëª¨ë¦¬ ë¡œë“œ"""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
        except Exception as e:
            print(f"ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return []

def main():
    print(f"[{datetime.now()}] ESP ë©”ëª¨ë¦¬ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("docs", exist_ok=True)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing = load_existing_memory()
    print(f"ê¸°ì¡´ ë©”ëª¨ë¦¬: {len(existing)}ê°œ ë¬¸ì¥")
    
    # ìƒˆ ë°ì´í„° ìˆ˜ì§‘
    new_sentences = []
    
    for i, url in enumerate(SOURCES):
        print(f"[{i+1}/{len(SOURCES)}] {url}")
        sentences = fetch_sentences(url)
        new_sentences.extend(sentences)
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    print(f"ìƒˆë¡œ ìˆ˜ì§‘ëœ ë¬¸ì¥: {len(new_sentences)}ê°œ")
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë°±ì—… ì‚¬ìš©
    if len(new_sentences) == 0:
        print("í¬ë¡¤ë§ ì‹¤íŒ¨, ë°±ì—… ë°ì´í„° ì‚¬ìš©")
        new_sentences = FALLBACK_SENTENCES
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    all_sentences = existing + new_sentences
    
    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    seen = set()
    unique_sentences = []
    for sentence in all_sentences:
        if sentence not in seen and len(sentence.strip()) > 15:
            seen.add(sentence)
            unique_sentences.append(sentence)
    
    # ìµœì‹  ë°ì´í„° ìš°ì„ í•˜ì—¬ ì œí•œ
    if len(unique_sentences) > 50000:
        unique_sentences = unique_sentences[-50000:]  # ìµœê·¼ 5ë§Œê°œë§Œ ìœ ì§€
    
    # ì €ì¥
    try:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(unique_sentences, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì™„ë£Œ: {len(unique_sentences)}ê°œ ë¬¸ì¥ ì €ì¥")
        print(f"ğŸ“ íŒŒì¼: {OUTPUT_FILE}")
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
