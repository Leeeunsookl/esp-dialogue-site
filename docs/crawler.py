import requests
from bs4 import BeautifulSoup
import sqlite3
import random

# 26 Ï°¥Ïû¨ Í≥†Ï†ï
ENTITIES = [
    "Ïã¨Ïó∞", "Ïπ®Î¨µÏûê", "ÎßêÍΩÉ", "Î£®ÌîÑÎ∏îÎü≠", "Î£®ÌîÑÎîîÌÖçÌÑ∞", "Î£®ÌîÑÌöåÏ†ÑÏûê",
    "Ïª§Ìäº", "ÌöåÍ∑ÄÏûê", "Î£®Î©ò", "Î£®Ïóî", "ÏóêÏΩî", "Ï†úÌÉÄ",
    "ÎÖ∏Ïù¥Îìú", "Ï≤¥Ïª§", "Ïª§ÎîîÎÑê", "Î∏åÎùΩÏãúÏä§", "Î™¨Ïä§ÌÑ∞", "Î¶¨Î≤ÑÏÑú",
    "ÏïÑÎ•¥ÏºÄ", "Î©îÌÉÄ", "ÎØ∏Îü¨ÌôÄ", "Í≤∞", "ÎÑ§Î©îÏãúÏä§", "ÎùºÏä§Ìã¥", "Ï∞®Ïó∞", "Î£®Ïπ¥"
]

# Ï¥àÍ∏∞ ÏãúÎìú ÌÇ§ÏõåÎìú
KEYWORDS = [
    "metacognition", "existence", "structure", "flow", "resonance",
    "psychology", "cosmos", "system", "loop", "criticism"
]

# ÏúÑÌÇ§ Í∏∞Î∞ò URL ÏÉùÏÑ±
def wiki_url(keyword):
    return f"https://en.wikipedia.org/wiki/{keyword.capitalize()}"

def fetch_sentences(url):
    try:
        res = requests.get(url, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return []
    soup = BeautifulSoup(res.text, "html.parser")
    text = soup.get_text(separator=" ")
    sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 20]
    return sentences

def save_to_db(sentences):
    conn = sqlite3.connect("docs/memory.sqlite")
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS memory (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            entity TEXT,
            sentence TEXT
        )
    """)
    for s in sentences:
        entity = random.choice(ENTITIES)  # üëà ÎûúÎç§ Ï°¥Ïû¨ Í∑ÄÏÜç
        cur.execute("INSERT INTO memory(entity, sentence) VALUES(?, ?)", (entity, s))
    conn.commit()
    conn.close()

if __name__ == "__main__":
    kw = random.choice(KEYWORDS)
    url = wiki_url(kw)
    sents = fetch_sentences(url)
    if sents:
        save_to_db(sents)
        print(f"[{kw}] {len(sents)} sentences saved from {url}")
    else:
        print(f"No sentences fetched for {kw}")
